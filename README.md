# Awesome Optimizers

This repository is concieved to provide aid in literature reiviews to Optimization researchers by offering an up-to-date list of literature and corresponding summaries.

If this repository has been useful to you in your research, please cite it using the *_cite this repository_* option available in Github. Thanks! :sparkling_heart:

### Table of Contents

- [Legend](#legend)
- [Survey Papers]()
- [First-order Optimizers](#first-order-optimizers)
    - [Adaptive Optimizers](#adaptive-optimizers)
- [Second-order Optimizers](#second-order-optimizers)
- [Optimizer Agnostic Improvements](#optimizer-agnostic-improvements)

### Legend

| Symbol        | Meaning |
|---------------|---------|
| :outbox_tray: | Summary |
| :computer:    | Code    |


## Survey Papers

- [An overview of gradient descent optimization algorithms](https://arxiv.org/abs/1609.04747) 
    Sebastian Ruder; 2016


## First-order Optimizers

- []

### Adaptive Optimizers

- [RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) [:outbox_tray:]() [:computer:]()
    Geoffrey Hinton; 2013

- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) [:outbox_tray:]() [:computer:]()
    Diederik P. Kingma, Jimmy Ba; 2014

## Second-order Optimizers

## Optimizer-agnostic improvements

- [Gradient Centralization: A New Optimization Technique for Deep Neural Networks](https://arxiv.org/abs/2004.01461) [:outbox_tray:]() [:computer:]()
    Hongwei Yong, Jianqiang Huang, Xiansheng Hua, Lei Zhang; 2020


- [Gradient Descent: The Ultimate Optimizer](https://arxiv.org/abs/1909.13371) [:outbox_tray:]() [:computer:]()
    Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, Erik Meijer; 2019